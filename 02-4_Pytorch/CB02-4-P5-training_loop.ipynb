{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CB02-4 Part Five: Typical Training Loop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 01 combining everything so far"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "class mlp(torch.nn.Module):\n",
    "    def __init__(self, num_inputs, num_outputs):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.layers = torch.nn.Sequential(\n",
    "            \n",
    "            # 1st hidden layer\n",
    "            torch.nn.Linear(num_inputs, 16),\n",
    "            torch.nn.ReLU(),\n",
    "\n",
    "            # 2nd hidden layer\n",
    "            torch.nn.Linear(16, 16),\n",
    "            torch.nn.ReLU(),\n",
    "\n",
    "            # output layer\n",
    "            torch.nn.Linear(16, num_outputs)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        logits = self.layers(x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = torch.tensor([\n",
    "    [-1.2, 3.1],\n",
    "    [-0.9, 2.9],\n",
    "    [-0.5, 2.6],\n",
    "    [2.3, -1.1],\n",
    "    [2.7, -1.5]\n",
    "])\n",
    "y_train = torch.tensor([0, 0, 0, 1, 1])\n",
    "\n",
    "x_test = torch.tensor([\n",
    "[-0.8, 2.8],\n",
    "[2.6, -1.6],\n",
    "])\n",
    "y_test = torch.tensor([0, 1])\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "class ToyDataset(Dataset):\n",
    "    def __init__(self, x, y):\n",
    "        self.features = x\n",
    "        self.labels = y\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.labels.shape[0] # self.features.shape[0]\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.features[idx], self.labels[idx]\n",
    "\n",
    "train_dataset = ToyDataset(x_train, y_train)\n",
    "test_dataset = ToyDataset(x_test, y_test)\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "torch.manual_seed(0)\n",
    "train_loader = DataLoader(\n",
    "    dataset=train_dataset, \n",
    "    batch_size=2, \n",
    "    shuffle=True,\n",
    "    num_workers= 0,\n",
    "    drop_last=True\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    dataset=test_dataset, \n",
    "    batch_size=2, \n",
    "    shuffle=False,\n",
    "    num_workers= 0,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 02 typical training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Batch: 0, Loss: 0.80\n",
      "Epoch: 0, Batch: 1, Loss: 0.82\n",
      "Epoch: 1, Batch: 0, Loss: 0.30\n",
      "Epoch: 1, Batch: 1, Loss: 0.22\n",
      "Epoch: 2, Batch: 0, Loss: 0.00\n",
      "Epoch: 2, Batch: 1, Loss: 0.00\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(0)\n",
    "model = mlp(2, 2)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1)\n",
    "\n",
    "num_epochs = 3\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "\n",
    "    model.train() # set the model to training mode: redundant for this example\n",
    "\n",
    "    for idx, (features, labels) in enumerate(train_loader):\n",
    "        # clear the gradients for every batch\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward pass\n",
    "        logits = model(features)\n",
    "\n",
    "        # compute the loss\n",
    "        loss = torch.nn.functional.cross_entropy(logits, labels)\n",
    "        \n",
    "        # backward pass\n",
    "        loss.backward()\n",
    "\n",
    "        # update weights & biases through SGD\n",
    "        optimizer.step()\n",
    "\n",
    "        print(f'Epoch: {epoch}, Batch: {idx}, Loss: {loss:.2f}')\n",
    "    \n",
    "    model.eval() # set the model to evaluation mode: redundant for this example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "mlp(\n",
       "  (layers): Sequential(\n",
       "    (0): Linear(in_features=2, out_features=30, bias=True)\n",
       "    (1): Sigmoid()\n",
       "    (2): Linear(in_features=30, out_features=20, bias=True)\n",
       "    (3): Sigmoid()\n",
       "    (4): Linear(in_features=20, out_features=2, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: \n",
    "\n",
    "1. In practicce, usually use validation dataset to find the optimal hyperparameter settings. \n",
    "A validation dataset is similar to a test set. However, \n",
    "while we only want to use a test set precisely once to avoid biasing the evaluation, \n",
    "we usually use the validation set multiple times to tweak the model settings.\n",
    "\n",
    "2. PREVENTING UNDESIRED GRADIENT ACCUMULATION It is important to include an\n",
    "optimizer.zero_grad() call in each update round to reset the gradients to zero. Otherwise, the\n",
    "gradients will accumulate, which may be undesired"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 03 Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 8.0681, -7.6774],\n",
       "        [ 7.3428, -6.9849],\n",
       "        [ 6.2796, -5.9794],\n",
       "        [-3.8422,  4.2457],\n",
       "        [-4.7082,  5.1806]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    output = model(x_train)\n",
    "output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "prob = torch.softmax(output, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(5)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = torch.argmax(prob, dim=1)\n",
    "torch.sum(predictions == y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    output = model(x_test)\n",
    "prob = torch.softmax(output, dim=1)\n",
    "predictions = torch.argmax(prob, dim=1)\n",
    "num_correct = torch.sum(predictions == y_test)\n",
    "\n",
    "num_correct.item() / len(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Note: Generated by Kimi\n",
    "\n",
    "Softmax 和 Cross-Entropy 是机器学习和深度学习中常用的概念，它们通常在分类问题中一起使用。\n",
    "\n",
    "### Softmax 函数\n",
    "Softmax 函数是一种将一个向量或一组实数转换成概率分布的函数。给定一个向量 \\( z \\)，其中 \\( z_i \\) 是向量中的第 \\( i \\) 个元素，Softmax 函数定义为：\n",
    "\n",
    "$\\text{Softmax}(z_i) = \\frac{e^{z_i}}{\\sum_{j} e^{z_j}}$\n",
    "\n",
    "其中，分母是对 \\( z \\) 中所有元素应用指数函数后求和的结果。Softmax 函数的输出是一个概率分布，每个元素的值都在 0 到 1 之间，并且所有元素的和为 1。\n",
    "\n",
    "### Cross-Entropy 损失\n",
    "Cross-Entropy 损失函数用于衡量两个概率分布之间的差异，通常用于分类问题中衡量模型预测的概率分布与真实标签的概率分布之间的差异。对于二分类问题，Cross-Entropy 损失可以表示为：\n",
    "\n",
    "$L = -\\left( y \\log(p) + (1 - y) \\log(1 - p) \\right)$\n",
    "\n",
    "其中，\\( y \\) 是真实标签（0 或 1），\\( p \\) 是模型预测样本为类别 1 的概率。\n",
    "\n",
    "对于多分类问题，Cross-Entropy 损失可以表示为：\n",
    "\n",
    "$L = -\\sum_{c=1}^{M} y_{o,c} \\log(p_{o,c})$\n",
    "\n",
    "其中，\\( M \\) 是类别的数量，\\( y_{o,c} \\) 是一个二进制指示器（0 或 1），如果类别 \\( c \\) 是样本 \\( o \\) 的正确分类，则为 1，否则为 0；\\( p_{o,c} \\) 是模型预测样本 \\( o \\) 属于类别 \\( c \\) 的概率。\n",
    "\n",
    "### Softmax 和 Cross-Entropy 的结合\n",
    "在多分类问题中，Softmax 函数通常用于模型的输出层，将模型的输出（通常是未归一化的预测值，称为 logits）转换成概率分布。然后，这个概率分布可以直接用于计算 Cross-Entropy 损失，以此来训练模型。通过最小化 Cross-Entropy 损失，模型学习调整其参数，以便预测的概率分布尽可能接近真实标签的概率分布。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 04 Model Save & load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'model_v1.pth') # .pth or .pt are the most common extensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_model = mlp(2, 2)\n",
    "new_model.load_state_dict(torch.load('model_v1.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_model.eval()\n",
    "with torch.no_grad():\n",
    "    output = new_model(x_train)\n",
    "prob = torch.softmax(output, dim=1)\n",
    "predictions = torch.argmax(prob, dim=1)\n",
    "num_correct = torch.sum(predictions == y_train)\n",
    "\n",
    "num_correct.item() / len(y_train)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "d2l",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
